<<<<<<< HEAD
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1927344,"status":"ok","timestamp":1762244659318,"user":{"displayName":"Anshav Desai","userId":"12679447231453701304"},"user_tz":-330},"id":"tmL0ts9TqQ0n","outputId":"6d0f4078-01f5-4a9e-b852-2fb1c64140d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","PROJECT_ROOT: /content/drive/MyDrive/aerial_project\n","CLASSIFICATION_DIR exists: True\n","Found 2662 images belonging to 2 classes.\n","Found 442 images belonging to 2 classes.\n","Found 215 images belonging to 2 classes.\n","Class names (saved): ['bird', 'drone']\n","Class Weights: {0: np.float64(0.9413012729844413), 1: np.float64(1.0665064102564104)}\n","Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,055,972\u001b[0m (15.47 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,055,972</span> (15.47 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,530,705\u001b[0m (9.65 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,530,705</span> (9.65 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,525,267\u001b[0m (5.82 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,525,267</span> (5.82 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8122 - loss: 0.4152"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 11s/step - accuracy: 0.8131 - loss: 0.4133 - val_accuracy: 0.9615 - val_loss: 0.1499 - learning_rate: 1.0000e-04\n","Epoch 2/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.9660 - loss: 0.0849"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 545ms/step - accuracy: 0.9661 - loss: 0.0847 - val_accuracy: 0.9819 - val_loss: 0.0719 - learning_rate: 1.0000e-04\n","Epoch 3/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - accuracy: 0.9779 - loss: 0.0670"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 545ms/step - accuracy: 0.9779 - loss: 0.0671 - val_accuracy: 0.9819 - val_loss: 0.0541 - learning_rate: 1.0000e-04\n","Epoch 4/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.9750 - loss: 0.0621"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 549ms/step - accuracy: 0.9750 - loss: 0.0620 - val_accuracy: 0.9864 - val_loss: 0.0452 - learning_rate: 1.0000e-04\n","Epoch 5/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 516ms/step - accuracy: 0.9908 - loss: 0.0305 - val_accuracy: 0.9842 - val_loss: 0.0488 - learning_rate: 1.0000e-04\n","Epoch 6/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - accuracy: 0.9930 - loss: 0.0242"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 535ms/step - accuracy: 0.9930 - loss: 0.0242 - val_accuracy: 0.9864 - val_loss: 0.0449 - learning_rate: 1.0000e-04\n","Epoch 7/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 534ms/step - accuracy: 0.9958 - loss: 0.0149 - val_accuracy: 0.9887 - val_loss: 0.0532 - learning_rate: 1.0000e-04\n","Epoch 8/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 509ms/step - accuracy: 0.9962 - loss: 0.0155 - val_accuracy: 0.9842 - val_loss: 0.0526 - learning_rate: 1.0000e-04\n","Epoch 9/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - accuracy: 0.9933 - loss: 0.0223\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 505ms/step - accuracy: 0.9933 - loss: 0.0223 - val_accuracy: 0.9842 - val_loss: 0.0579 - learning_rate: 1.0000e-04\n","Epoch 10/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 501ms/step - accuracy: 0.9926 - loss: 0.0240 - val_accuracy: 0.9842 - val_loss: 0.0530 - learning_rate: 3.0000e-05\n","Epoch 11/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 511ms/step - accuracy: 0.9989 - loss: 0.0073 - val_accuracy: 0.9887 - val_loss: 0.0512 - learning_rate: 3.0000e-05\n","Epoch 1/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961ms/step - accuracy: 0.9715 - loss: 0.0878"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 1s/step - accuracy: 0.9715 - loss: 0.0877 - val_accuracy: 0.9910 - val_loss: 0.0401 - learning_rate: 1.0000e-05\n","Epoch 2/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 541ms/step - accuracy: 0.9730 - loss: 0.0767 - val_accuracy: 0.9819 - val_loss: 0.0479 - learning_rate: 1.0000e-05\n","Epoch 3/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 526ms/step - accuracy: 0.9739 - loss: 0.0743 - val_accuracy: 0.9819 - val_loss: 0.0534 - learning_rate: 1.0000e-05\n","Epoch 4/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.9678 - loss: 0.0906\n","Epoch 4: ReduceLROnPlateau reducing learning rate to 2.9999999242136253e-06.\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 520ms/step - accuracy: 0.9679 - loss: 0.0903 - val_accuracy: 0.9796 - val_loss: 0.0580 - learning_rate: 1.0000e-05\n","Epoch 5/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 520ms/step - accuracy: 0.9797 - loss: 0.0573 - val_accuracy: 0.9796 - val_loss: 0.0615 - learning_rate: 3.0000e-06\n","Epoch 6/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 531ms/step - accuracy: 0.9831 - loss: 0.0425 - val_accuracy: 0.9796 - val_loss: 0.0618 - learning_rate: 3.0000e-06\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}],"source":["#Colab: 04_train_transfer.ipynb\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","import tensorflow as tf\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from sklearn.utils import class_weight\n","import numpy as np\n","import json\n","import pickle\n","#Project Paths\n","PROJECT_ROOT='/content/drive/MyDrive/aerial_project'\n","CLASSIFICATION_DIR=os.path.join(PROJECT_ROOT, 'classification_dataset')\n","SAVED_MODELS=os.path.join(PROJECT_ROOT, 'saved_models')\n","RESULTS_DIR=os.path.join(PROJECT_ROOT, 'results')\n","\n","os.makedirs(SAVED_MODELS, exist_ok=True)\n","os.makedirs(RESULTS_DIR, exist_ok=True)\n","\n","print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n","print(\"CLASSIFICATION_DIR exists:\", os.path.exists(CLASSIFICATION_DIR))\n","#Image settings\n","IMG_SIZE=(224, 224)\n","BATCH_SIZE=32\n","#Data augmentation\n","train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.15,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","#validation generator\n","val_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=preprocess_input) #preprocess_input: to scale images correctly which we use for efficientnet\n","\n","#Data loaders\n","train_ds=train_datagen.flow_from_directory(\n","    os.path.join(CLASSIFICATION_DIR, 'train'),\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary')\n","\n","val_ds=val_datagen.flow_from_directory(\n","    os.path.join(CLASSIFICATION_DIR, 'valid'),\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    shuffle=False)\n","\n","test_ds=val_datagen.flow_from_directory(\n","    os.path.join(CLASSIFICATION_DIR, 'test'),\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    shuffle=False) #class_mode is binary for bird and drone classif.\n","\n","#Saving the class names for streamlit\n","class_names=list(train_ds.class_indices.keys())\n","print(\"Class names (saved):\", class_names)\n","with open(os.path.join(SAVED_MODELS, 'class_names.json'), 'w') as f:\n","    json.dump(class_names, f)\n","\n","#computing the class weights\n","labels=train_ds.classes\n","class_weights_vals=class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(labels),\n","    y=labels)\n","class_weights=dict(enumerate(class_weights_vals))\n","print(\"Class Weights:\", class_weights)\n","\n","#Building and compiling the model\n","base_model=EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet') #include_top is false to load without top classifier\n","#unfreezing the last 50 layers for fine-tuning\n","for layer in base_model.layers[:-50]:\n","    layer.trainable=False\n","\n","inputs=tf.keras.Input(shape=(224,224,3))\n","x=base_model(inputs, training=False)\n","x=layers.GlobalAveragePooling2D()(x) #gap2d reduces feature maps to a vector\n","x=layers.BatchNormalization()(x)\n","x=layers.Dropout(0.4)(x) #BN and Dropout prevent the overfitting\n","outputs=layers.Dense(1, activation='sigmoid')(x)  #sigmoid for binary class.\n","\n","model=models.Model(inputs, outputs)\n","model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","#Callbacks\n","callbacks=[\n","    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n","    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1, min_lr=1e-6),\n","    ModelCheckpoint(os.path.join(SAVED_MODELS, 'best_efficientnet_finetuned.h5'), save_best_only=True)]\n","#earlystop to stop train. when val. loss stops improving. Model Check. saves the best weights and ReduceLR lowers learning rate when stuck\n","\n","#Training\n","history=model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=20,\n","    class_weight=class_weights,\n","    callbacks=callbacks)\n","\n","#Fine-tuning (Unfreezing everything and training with a very low LR, 1e-5)\n","for layer in base_model.layers:\n","    layer.trainable=True\n","\n","model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n","history_finetune=model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=10,\n","    class_weight=class_weights,\n","    callbacks=callbacks)\n","\n","#Saving the model and the training history\n","with open(os.path.join(SAVED_MODELS, 'history_efficientnet.pkl'), 'wb') as f:\n","    pickle.dump({'initial': history.history, 'finetune': history_finetune.history}, f)\n","model.save(os.path.join(SAVED_MODELS, 'efficientnet_finetuned_full.h5'))"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122196,"status":"ok","timestamp":1762244781521,"user":{"displayName":"Anshav Desai","userId":"12679447231453701304"},"user_tz":-330},"id":"VIy8e_buUIRg","outputId":"19783935-8e3e-4cf0-9f19-a94f96ca4591"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 14s/step\n","              precision    recall  f1-score   support\n","\n","        bird       0.98      0.99      0.99       121\n","       drone       0.99      0.98      0.98        94\n","\n","    accuracy                           0.99       215\n","   macro avg       0.99      0.99      0.99       215\n","weighted avg       0.99      0.99      0.99       215\n","\n","Confusion Matrix:\n"," [[120   1]\n"," [  2  92]]\n"]}],"source":["#Manual evaluation metrics (after training)\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np\n","#get predictions\n","y_true=test_ds.classes\n","y_pred_probs=model.predict(test_ds)\n","y_pred=(y_pred_probs > 0.5).astype(int).flatten() #threshold is set 0.5\n","#classification report\n","print(classification_report(y_true,y_pred,target_names=class_names))\n","#confusion mat.\n","cm=confusion_matrix(y_true,y_pred)\n","print(\"Confusion Matrix:\\n\",cm)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPl0cK9ynBZLlsCH/IhrUXR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
=======
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1927344,"status":"ok","timestamp":1762244659318,"user":{"displayName":"Anshav Desai","userId":"12679447231453701304"},"user_tz":-330},"id":"tmL0ts9TqQ0n","outputId":"6d0f4078-01f5-4a9e-b852-2fb1c64140d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","PROJECT_ROOT: /content/drive/MyDrive/aerial_project\n","CLASSIFICATION_DIR exists: True\n","Found 2662 images belonging to 2 classes.\n","Found 442 images belonging to 2 classes.\n","Found 215 images belonging to 2 classes.\n","Class names (saved): ['bird', 'drone']\n","Class Weights: {0: np.float64(0.9413012729844413), 1: np.float64(1.0665064102564104)}\n","Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n","\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,055,972\u001b[0m (15.47 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,055,972</span> (15.47 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,530,705\u001b[0m (9.65 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,530,705</span> (9.65 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,525,267\u001b[0m (5.82 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,525,267</span> (5.82 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8122 - loss: 0.4152"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m972s\u001b[0m 11s/step - accuracy: 0.8131 - loss: 0.4133 - val_accuracy: 0.9615 - val_loss: 0.1499 - learning_rate: 1.0000e-04\n","Epoch 2/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step - accuracy: 0.9660 - loss: 0.0849"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 545ms/step - accuracy: 0.9661 - loss: 0.0847 - val_accuracy: 0.9819 - val_loss: 0.0719 - learning_rate: 1.0000e-04\n","Epoch 3/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - accuracy: 0.9779 - loss: 0.0670"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 545ms/step - accuracy: 0.9779 - loss: 0.0671 - val_accuracy: 0.9819 - val_loss: 0.0541 - learning_rate: 1.0000e-04\n","Epoch 4/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.9750 - loss: 0.0621"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 549ms/step - accuracy: 0.9750 - loss: 0.0620 - val_accuracy: 0.9864 - val_loss: 0.0452 - learning_rate: 1.0000e-04\n","Epoch 5/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 516ms/step - accuracy: 0.9908 - loss: 0.0305 - val_accuracy: 0.9842 - val_loss: 0.0488 - learning_rate: 1.0000e-04\n","Epoch 6/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - accuracy: 0.9930 - loss: 0.0242"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 535ms/step - accuracy: 0.9930 - loss: 0.0242 - val_accuracy: 0.9864 - val_loss: 0.0449 - learning_rate: 1.0000e-04\n","Epoch 7/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 534ms/step - accuracy: 0.9958 - loss: 0.0149 - val_accuracy: 0.9887 - val_loss: 0.0532 - learning_rate: 1.0000e-04\n","Epoch 8/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 509ms/step - accuracy: 0.9962 - loss: 0.0155 - val_accuracy: 0.9842 - val_loss: 0.0526 - learning_rate: 1.0000e-04\n","Epoch 9/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - accuracy: 0.9933 - loss: 0.0223\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 505ms/step - accuracy: 0.9933 - loss: 0.0223 - val_accuracy: 0.9842 - val_loss: 0.0579 - learning_rate: 1.0000e-04\n","Epoch 10/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 501ms/step - accuracy: 0.9926 - loss: 0.0240 - val_accuracy: 0.9842 - val_loss: 0.0530 - learning_rate: 3.0000e-05\n","Epoch 11/20\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 511ms/step - accuracy: 0.9989 - loss: 0.0073 - val_accuracy: 0.9887 - val_loss: 0.0512 - learning_rate: 3.0000e-05\n","Epoch 1/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961ms/step - accuracy: 0.9715 - loss: 0.0878"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 1s/step - accuracy: 0.9715 - loss: 0.0877 - val_accuracy: 0.9910 - val_loss: 0.0401 - learning_rate: 1.0000e-05\n","Epoch 2/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 541ms/step - accuracy: 0.9730 - loss: 0.0767 - val_accuracy: 0.9819 - val_loss: 0.0479 - learning_rate: 1.0000e-05\n","Epoch 3/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 526ms/step - accuracy: 0.9739 - loss: 0.0743 - val_accuracy: 0.9819 - val_loss: 0.0534 - learning_rate: 1.0000e-05\n","Epoch 4/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.9678 - loss: 0.0906\n","Epoch 4: ReduceLROnPlateau reducing learning rate to 2.9999999242136253e-06.\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 520ms/step - accuracy: 0.9679 - loss: 0.0903 - val_accuracy: 0.9796 - val_loss: 0.0580 - learning_rate: 1.0000e-05\n","Epoch 5/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 520ms/step - accuracy: 0.9797 - loss: 0.0573 - val_accuracy: 0.9796 - val_loss: 0.0615 - learning_rate: 3.0000e-06\n","Epoch 6/10\n","\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 531ms/step - accuracy: 0.9831 - loss: 0.0425 - val_accuracy: 0.9796 - val_loss: 0.0618 - learning_rate: 3.0000e-06\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}],"source":["#Colab: 04_train_transfer.ipynb\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","import tensorflow as tf\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from sklearn.utils import class_weight\n","import numpy as np\n","import json\n","import pickle\n","#Project Paths\n","PROJECT_ROOT='/content/drive/MyDrive/aerial_project'\n","CLASSIFICATION_DIR=os.path.join(PROJECT_ROOT, 'classification_dataset')\n","SAVED_MODELS=os.path.join(PROJECT_ROOT, 'saved_models')\n","RESULTS_DIR=os.path.join(PROJECT_ROOT, 'results')\n","\n","os.makedirs(SAVED_MODELS, exist_ok=True)\n","os.makedirs(RESULTS_DIR, exist_ok=True)\n","\n","print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n","print(\"CLASSIFICATION_DIR exists:\", os.path.exists(CLASSIFICATION_DIR))\n","#Image settings\n","IMG_SIZE=(224, 224)\n","BATCH_SIZE=32\n","#Data augmentation\n","train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.15,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","val_datagen=tf.keras.preprocessing.image.ImageDataGenerator(\n","    preprocessing_function=preprocess_input)\n","\n","#Data loaders\n","train_ds=train_datagen.flow_from_directory(\n","    os.path.join(CLASSIFICATION_DIR, 'train'),\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary')\n","\n","val_ds=val_datagen.flow_from_directory(\n","    os.path.join(CLASSIFICATION_DIR, 'valid'),\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    shuffle=False)\n","\n","test_ds=val_datagen.flow_from_directory(\n","    os.path.join(CLASSIFICATION_DIR, 'test'),\n","    target_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    shuffle=False)\n","\n","#Saving the class names for streamlit\n","class_names=list(train_ds.class_indices.keys())\n","print(\"Class names (saved):\", class_names)\n","with open(os.path.join(SAVED_MODELS, 'class_names.json'), 'w') as f:\n","    json.dump(class_names, f)\n","\n","#computing the class weights\n","labels=train_ds.classes\n","class_weights_vals=class_weight.compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(labels),\n","    y=labels)\n","class_weights=dict(enumerate(class_weights_vals))\n","print(\"Class Weights:\", class_weights)\n","\n","#Building and compiling the model\n","base_model=EfficientNetB0(include_top=False, input_shape=(224,224,3), weights='imagenet')\n","#unfreezing the last 50 layers for fine-tuning\n","for layer in base_model.layers[:-50]:\n","    layer.trainable=False\n","\n","inputs=tf.keras.Input(shape=(224,224,3))\n","x=base_model(inputs, training=False)\n","x=layers.GlobalAveragePooling2D()(x)\n","x=layers.BatchNormalization()(x)\n","x=layers.Dropout(0.4)(x)\n","outputs=layers.Dense(1, activation='sigmoid')(x)\n","\n","model=models.Model(inputs, outputs)\n","model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","#Callbacks\n","callbacks=[\n","    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n","    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1, min_lr=1e-6),\n","    ModelCheckpoint(os.path.join(SAVED_MODELS, 'best_efficientnet_finetuned.h5'), save_best_only=True)]\n","\n","#Training\n","history=model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=20,\n","    class_weight=class_weights,\n","    callbacks=callbacks)\n","\n","#Fine-tuning (Unfreezing everything and training with a very low LR)\n","for layer in base_model.layers:\n","    layer.trainable=True\n","\n","model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n","history_finetune=model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=10,\n","    class_weight=class_weights,\n","    callbacks=callbacks)\n","\n","#Saving the model and the training history\n","with open(os.path.join(SAVED_MODELS, 'history_efficientnet.pkl'), 'wb') as f:\n","    pickle.dump({'initial': history.history, 'finetune': history_finetune.history}, f)\n","model.save(os.path.join(SAVED_MODELS, 'efficientnet_finetuned_full.h5'))"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122196,"status":"ok","timestamp":1762244781521,"user":{"displayName":"Anshav Desai","userId":"12679447231453701304"},"user_tz":-330},"id":"VIy8e_buUIRg","outputId":"19783935-8e3e-4cf0-9f19-a94f96ca4591"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 14s/step\n","              precision    recall  f1-score   support\n","\n","        bird       0.98      0.99      0.99       121\n","       drone       0.99      0.98      0.98        94\n","\n","    accuracy                           0.99       215\n","   macro avg       0.99      0.99      0.99       215\n","weighted avg       0.99      0.99      0.99       215\n","\n","Confusion Matrix:\n"," [[120   1]\n"," [  2  92]]\n"]}],"source":["#Manual evaluation metrics (after training)\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np\n","#get predictions\n","y_true=test_ds.classes\n","y_pred_probs=model.predict(test_ds)\n","y_pred=(y_pred_probs > 0.5).astype(int).flatten()\n","#classification report\n","print(classification_report(y_true,y_pred,target_names=class_names))\n","#confusion mat.\n","cm=confusion_matrix(y_true,y_pred)\n","print(\"Confusion Matrix:\\n\",cm)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOoOMWl5I85gNsU/ONar9JG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
>>>>>>> b821929c5a3060ce5d8c42896a5696c1f7acebe2
